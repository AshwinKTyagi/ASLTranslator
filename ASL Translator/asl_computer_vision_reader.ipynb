{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YfsPXgkxFgnO"
   },
   "source": [
    "# Live Sign Language Translator\n",
    "This notebook will take video input from the user and transform it into a list of photos. It will then find what the asl equiv is to the inputted hand sign "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 174,
     "status": "ok",
     "timestamp": 1661889511695,
     "user": {
      "displayName": "Ashwin Tyagi",
      "userId": "01848559523199568635"
     },
     "user_tz": 420
    },
    "id": "JUq2cSlLF7h1",
    "outputId": "adbe4024-1796-4cd9-a590-9ebb36283967"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on Hands in module mediapipe.python.solutions.hands object:\n",
      "\n",
      "class Hands(mediapipe.python.solution_base.SolutionBase)\n",
      " |  Hands(static_image_mode=False, max_num_hands=2, model_complexity=1, min_detection_confidence=0.5, min_tracking_confidence=0.5)\n",
      " |  \n",
      " |  MediaPipe Hands.\n",
      " |  \n",
      " |  MediaPipe Hands processes an RGB image and returns the hand landmarks and\n",
      " |  handedness (left v.s. right hand) of each detected hand.\n",
      " |  \n",
      " |  Note that it determines handedness assuming the input image is mirrored,\n",
      " |  i.e., taken with a front-facing/selfie camera (\n",
      " |  https://en.wikipedia.org/wiki/Front-facing_camera) with images flipped\n",
      " |  horizontally. If that is not the case, use, for instance, cv2.flip(image, 1)\n",
      " |  to flip the image first for a correct handedness output.\n",
      " |  \n",
      " |  Please refer to https://solutions.mediapipe.dev/hands#python-solution-api for\n",
      " |  usage examples.\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      Hands\n",
      " |      mediapipe.python.solution_base.SolutionBase\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, static_image_mode=False, max_num_hands=2, model_complexity=1, min_detection_confidence=0.5, min_tracking_confidence=0.5)\n",
      " |      Initializes a MediaPipe Hand object.\n",
      " |      \n",
      " |      Args:\n",
      " |        static_image_mode: Whether to treat the input images as a batch of static\n",
      " |          and possibly unrelated images, or a video stream. See details in\n",
      " |          https://solutions.mediapipe.dev/hands#static_image_mode.\n",
      " |        max_num_hands: Maximum number of hands to detect. See details in\n",
      " |          https://solutions.mediapipe.dev/hands#max_num_hands.\n",
      " |        model_complexity: Complexity of the hand landmark model: 0 or 1.\n",
      " |          Landmark accuracy as well as inference latency generally go up with the\n",
      " |          model complexity. See details in\n",
      " |          https://solutions.mediapipe.dev/hands#model_complexity.\n",
      " |        min_detection_confidence: Minimum confidence value ([0.0, 1.0]) for hand\n",
      " |          detection to be considered successful. See details in\n",
      " |          https://solutions.mediapipe.dev/hands#min_detection_confidence.\n",
      " |        min_tracking_confidence: Minimum confidence value ([0.0, 1.0]) for the\n",
      " |          hand landmarks to be considered tracked successfully. See details in\n",
      " |          https://solutions.mediapipe.dev/hands#min_tracking_confidence.\n",
      " |  \n",
      " |  process(self, image: numpy.ndarray) -> <function NamedTuple at 0x10485cb80>\n",
      " |      Processes an RGB image and returns the hand landmarks and handedness of each detected hand.\n",
      " |      \n",
      " |      Args:\n",
      " |        image: An RGB image represented as a numpy ndarray.\n",
      " |      \n",
      " |      Raises:\n",
      " |        RuntimeError: If the underlying graph throws any error.\n",
      " |        ValueError: If the input image is not three channel RGB.\n",
      " |      \n",
      " |      Returns:\n",
      " |        A NamedTuple object with the following fields:\n",
      " |          1) a \"multi_hand_landmarks\" field that contains the hand landmarks on\n",
      " |             each detected hand.\n",
      " |          2) a \"multi_hand_world_landmarks\" field that contains the hand landmarks\n",
      " |             on each detected hand in real-world 3D coordinates that are in meters\n",
      " |             with the origin at the hand's approximate geometric center.\n",
      " |          3) a \"multi_handedness\" field that contains the handedness (left v.s.\n",
      " |             right hand) of the detected hand.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from mediapipe.python.solution_base.SolutionBase:\n",
      " |  \n",
      " |  __enter__(self)\n",
      " |      A \"with\" statement support.\n",
      " |  \n",
      " |  __exit__(self, exc_type, exc_val, exc_tb)\n",
      " |      Closes all the input sources and the graph.\n",
      " |  \n",
      " |  close(self) -> None\n",
      " |      Closes all the input sources and the graph.\n",
      " |  \n",
      " |  create_graph_options(self, options_message: google.protobuf.message.Message, values: Mapping[str, Any]) -> google.protobuf.message.Message\n",
      " |      Sets protobuf field values.\n",
      " |      \n",
      " |      Args:\n",
      " |        options_message: the options protobuf message.\n",
      " |        values: field value pairs, where each field may be a \".\" separated path.\n",
      " |      \n",
      " |      Returns:\n",
      " |        the options protobuf message.\n",
      " |  \n",
      " |  reset(self) -> None\n",
      " |      Resets the graph for another run.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from mediapipe.python.solution_base.SolutionBase:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1705452792.506298       1 gl_context.cc:344] GL version: 2.1 (2.1 Metal - 83.1), renderer: Apple M2\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "cap = cv2.VideoCapture(0)\n",
    "mp_hands = mp.solutions.hands\n",
    "hands = mp_hands.Hands()\n",
    "mp_draw = mp.solutions.drawing_utils\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "help(hands)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7ovTBf72K-P9"
   },
   "source": [
    "#### Capture Image input and proccess it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 235
    },
    "executionInfo": {
     "elapsed": 406,
     "status": "error",
     "timestamp": 1661887221506,
     "user": {
      "displayName": "Ashwin Tyagi",
      "userId": "01848559523199568635"
     },
     "user_tz": 420
    },
    "id": "mYKd0xV8K6RB",
    "outputId": "a7d874a9-e678-4401-9eb9-df1190f107ca"
   },
   "outputs": [],
   "source": [
    "while True:\n",
    "    success, image = cap.read()\n",
    "    image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    results = hands.process(image_rgb)\n",
    "    \n",
    "    if results.multi_hand_landmarks != None:\n",
    "        landmarks = results.multi_hand_landmarks[0].landmark\n",
    "        \n",
    "        x_coords = np.array([lm.x for lm in landmarks])\n",
    "        y_coords = np.array([lm.y for lm in landmarks])\n",
    "        \n",
    "        # Calculate bounding box\n",
    "        margin = 50\n",
    "        x_min = max(0, int(np.min(x_coords) * image.shape[1]) - margin)\n",
    "        y_min = max(0, int(np.min(y_coords) * image.shape[0]) - margin)\n",
    "        x_max = min(image.shape[1], int(np.max(x_coords) * image.shape[1]) + margin)\n",
    "        y_max = min(image.shape[0], int(np.max(y_coords) * image.shape[0]) + margin)\n",
    "        \n",
    "        # Ensure a square bounding box\n",
    "        length = max(x_max - x_min, y_max - y_min)\n",
    "        x_max = min(image.shape[1], x_min + length)\n",
    "        y_max = min(image.shape[0], y_min + length)\n",
    "        # Crop the image to the bounding box\n",
    "        cropped_image = image[y_min:y_max, x_min:x_max]\n",
    "        cropped_image = cv2.resize(cropped_image,(128, 128), interpolation=cv2.INTER_CUBIC)\n",
    "\n",
    "        for hand_landmarks in results.multi_hand_landmarks:\n",
    "            \n",
    "            for id, lm, in enumerate(hand_landmarks.landmark):\n",
    "                h, w, c = image.shape\n",
    "                cx, cy = int(lm.x * w), int(lm.y * h)\n",
    "                \n",
    "                if id == 20:\n",
    "                    cv2.circle(image, (cx, cy), 25, (255, 0, 255), cv2.FILLED)\n",
    "    \n",
    "                mp_draw.draw_landmarks(image, hand_landmarks, mp_hands.HAND_CONNECTIONS)\n",
    "            cv2.imshow(\"Output\", image)\n",
    "            cv2.imshow(\"Cropped\", cropped_image)\n",
    "            cv2.waitKey(10)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(results.multi_hand_landmarks)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyNL4NWUnJ0RxpRpDyE8VGuB",
   "collapsed_sections": [],
   "name": "asl_computer_vision_reader.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "python3.11.6",
   "language": "python",
   "name": "python3.11.6"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
